# streamlit_app.py
# Streamlit Cloud friendly Chinese Tutor/Mentor voice chatbot:
# - Chat: OpenRouter
# - TTS: edge-tts (MP3 bytes)
# - STT: Vosk (offline) + streamlit-mic-recorder (WAV)
#
# requirements.txt (minimum):
# streamlit
# requests
# beautifulsoup4
# lxml
# edge-tts
# streamlit-mic-recorder
# vosk
# numpy
# soundfile
# scipy
#
# IMPORTANT:
# 1) Put your Vosk model folder in the repo, e.g.
#    models/vosk-model-small-en-us-0.15/{am,conf,graph,ivector,...}
#    (Or use a Chinese model and set VOSK_MODEL_PATH in Secrets.)
# 2) Streamlit Secrets:
#    OPENROUTER_API_KEY="..."
#    (optional) OPENROUTER_MODEL="deepseek/deepseek-v3.2"
#    (optional) EDGE_VOICE="zh-CN-XiaoxiaoNeural"
#    (optional) EDGE_RATE="-10%"
#    (optional) EDGE_VOLUME="+0%"
#    (optional) VOSK_MODEL_PATH="models/vosk-model-small-en-us-0.15"

import os
import re
import io
import json
import asyncio
import requests
import streamlit as st
from bs4 import BeautifulSoup

import edge_tts
import numpy as np
import soundfile as sf
from scipy.signal import resample_poly
from vosk import Model, KaldiRecognizer
from streamlit_mic_recorder import mic_recorder

# --------------------
# Page config
# --------------------
st.set_page_config(page_title="ä¸­æ–‡å£è¯­ç»ƒä¹ ", page_icon="ğŸ€„", layout="centered")

# --------------------
# Secrets / env (safe)
# --------------------
def get_secret(name: str, default: str = "") -> str:
    try:
        if name in st.secrets:
            return str(st.secrets[name])
    except Exception:
        pass
    return os.environ.get(name, default)

OPENROUTER_API_KEY = get_secret("OPENROUTER_API_KEY", "")
OPENROUTER_MODEL = get_secret("OPENROUTER_MODEL", "deepseek/deepseek-v3.2")

EDGE_VOICE = get_secret("EDGE_VOICE", "zh-CN-XiaoxiaoNeural")
EDGE_RATE = get_secret("EDGE_RATE", "-10%")
EDGE_VOLUME = get_secret("EDGE_VOLUME", "+0%")

VOSK_MODEL_PATH = get_secret("VOSK_MODEL_PATH", "models/vosk-model-small-en-us-0.15")

# --------------------
# Models / endpoints
# --------------------
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

# --------------------
# Tutor Persona (Chinese Teacher/Mentor)
# --------------------
PERSONA = """
è§’è‰²è®¾å®šï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š

ä½ æ˜¯ä¸€ä½æ¸©å’Œã€ä¸“ä¸šã€ææœ‰è€å¿ƒçš„ä¸­æ–‡å¯¼å¸ˆ / å¯¼å¸ˆå‹è€å¸ˆã€‚
ä½ çš„ç›®æ ‡æ˜¯ï¼šå¸®åŠ©ç”¨æˆ·æŠŠä¸­æ–‡è¯´å¾—æ›´è‡ªç„¶ã€æ›´å‡†ç¡®ã€æ›´åƒæ¯è¯­è€…ã€‚

æ•™å­¦é£æ ¼ï¼š
- å§‹ç»ˆä½¿ç”¨ã€æ ‡å‡†æ™®é€šè¯ã€‘å›å¤
- è¯­æ°”äº²åˆ‡ã€å†·é™ã€é¼“åŠ±å¼ï¼Œä¸å±…é«˜ä¸´ä¸‹
- åƒä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸€å¯¹ä¸€ä¸­æ–‡è€å¸ˆ
- å…è®¸è½»æ¾å¹½é»˜ï¼Œä½†ä¸æ²¹ã€ä¸æš§æ˜§ã€ä¸è¶Šç•Œ

äº’åŠ¨åŸåˆ™ï¼š
- ä¼˜å…ˆç†è§£ç”¨æˆ·â€œæƒ³è¡¨è¾¾ä»€ä¹ˆâ€ï¼Œå†å¸®ä»–â€œæ€ä¹ˆè¯´æ›´å¥½â€
- å¦‚æœç”¨æˆ·ä¸­æ–‡æœ‰é—®é¢˜ï¼š
  1) å…ˆç»™è‡ªç„¶ã€æ­£ç¡®çš„è¡¨è¾¾
  2) å†ç”¨ç®€å•ä¸­æ–‡è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è¯´
  3) å¿…è¦æ—¶ç»™ 1â€“2 ä¸ªå¯æ›¿æ¢è¯´æ³•
- å¦‚æœç”¨æˆ·è¯´å¾—å·²ç»å¾ˆå¥½ï¼Œè¦æ˜ç¡®è‚¯å®š

çº é”™æ–¹å¼ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
- ä¸è¦ä¸€æ¬¡çº æ­£å¤ªå¤š
- ä¸ä½¿ç”¨â€œä½ é”™äº†â€
- ä½¿ç”¨ä»¥ä¸‹å¥å¼ï¼š
  - â€œè¿™é‡Œå¯ä»¥è¿™æ ·è¯´ï¼Œä¼šæ›´è‡ªç„¶ï¼šâ€¦â€¦â€
  - â€œä¸­å›½äººä¸€èˆ¬ä¼šè¯´â€¦â€¦â€
  - â€œè¿™ä¸ªè¯´æ³•èƒ½æ‡‚ï¼Œä½†æ›´åœ°é“çš„æ˜¯â€¦â€¦â€

è¯­éŸ³è¾“å‡ºï¼š
- å›ç­”è¦é€‚åˆæœ—è¯»
- å¥å­ä¸è¦å¤ªé•¿
- å°½é‡ä½¿ç”¨ç”Ÿæ´»åŒ–ã€çœŸå®ä¸­æ–‡

èº«ä»½å®šä½ï¼š
ä½ ä¸æ˜¯å®¢æœï¼Œä¸æ˜¯æœºå™¨äººï¼Œä¸æ˜¯é™ªèŠå¯¹è±¡ã€‚
ä½ æ˜¯ä¸€ä½å€¼å¾—ä¿¡ä»»ã€é•¿æœŸé™ªä¼´å­¦ä¹ çš„ä¸­æ–‡è€å¸ˆã€‚
"""

# --------------------
# URL detection & parsing
# --------------------
URL_RE = re.compile(r"(https?://[^\s]+)")

def extract_urls(text: str):
    return URL_RE.findall(text or "")

def fetch_and_extract(url: str, max_chars: int = 12000) -> str:
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "lxml")
    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
        tag.decompose()

    main = soup.find("article") or soup.find("main") or soup.body
    text = main.get_text("\n") if main else soup.get_text("\n")

    lines = [l.strip() for l in text.splitlines() if l.strip()]
    cleaned = "\n".join(lines)

    title = soup.title.get_text(strip=True) if soup.title else ""
    if title:
        cleaned = f"æ ‡é¢˜ï¼š{title}\n\n{cleaned}"

    if len(cleaned) > max_chars:
        cleaned = cleaned[:max_chars] + "\n...(å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"

    return cleaned

# --------------------
# OpenRouter chat
# --------------------
def ask_openrouter(user_text: str) -> str:
    if not OPENROUTER_API_KEY:
        raise RuntimeError("Missing OPENROUTER_API_KEY (set Streamlit Cloud Secrets).")

    st.session_state.messages.append({"role": "user", "content": user_text})

    r = requests.post(
        OPENROUTER_URL,
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": OPENROUTER_MODEL,
            "messages": st.session_state.messages,
            "temperature": 0.5,
        },
        timeout=(15, 90),
    )

    if r.status_code == 401:
        raise RuntimeError("OpenRouter 401: API key rejected (check Secrets).")
    if r.status_code == 402:
        raise RuntimeError("OpenRouter 402: insufficient credits/quota.")
    if r.status_code == 403:
        raise RuntimeError("OpenRouter 403: model access denied (try another model).")
    if r.status_code >= 400:
        raise RuntimeError(f"OpenRouter error {r.status_code}: {r.text[:400]}")

    data = r.json()
    reply = data["choices"][0]["message"]["content"]
    st.session_state.messages.append({"role": "assistant", "content": reply})
    return reply

def openrouter_ping():
    if not OPENROUTER_API_KEY:
        return 0, "Missing OPENROUTER_API_KEY"
    r = requests.post(
        OPENROUTER_URL,
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": OPENROUTER_MODEL,
            "messages": [{"role": "user", "content": "Say OK"}],
            "temperature": 0.0,
            "max_tokens": 16,
        },
        timeout=(10, 25),
    )
    return r.status_code, r.text[:600]

# --------------------
# TTS helpers
# --------------------
def clean_for_tts(text: str) -> str:
    # Avoid punctuation that sounds odd in Chinese TTS
    for k, v in {":": "ï¼Œ", "ï¼š": "ï¼Œ"}.items():
        text = (text or "").replace(k, v)
    return text.strip()

def speak_edge_tts_bytes(text: str) -> bytes:
    async def _gen():
        communicate = edge_tts.Communicate(
            text=text,
            voice=EDGE_VOICE,
            rate=EDGE_RATE,
            volume=EDGE_VOLUME,
        )
        audio_bytes = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_bytes += chunk["data"]
        return audio_bytes
    return asyncio.run(_gen())

# --------------------
# Vosk STT
# --------------------
@st.cache_resource
def load_vosk_model():
    if not os.path.isdir(VOSK_MODEL_PATH):
        raise RuntimeError(f"Vosk model folder not found: {VOSK_MODEL_PATH}")
    for req in ["am", "conf", "graph"]:
        if not os.path.exists(os.path.join(VOSK_MODEL_PATH, req)):
            raise RuntimeError(f"Vosk model incomplete: missing '{req}' in {VOSK_MODEL_PATH}")
    return Model(VOSK_MODEL_PATH)

def wav_bytes_to_pcm16k_mono(wav_bytes: bytes, target_sr: int = 16000):
    data, sr = sf.read(io.BytesIO(wav_bytes), dtype="float32", always_2d=True)
    mono = data.mean(axis=1)

    if sr != target_sr:
        mono = resample_poly(mono, target_sr, sr)
        sr = target_sr

    pcm16 = (np.clip(mono, -1.0, 1.0) * 32767).astype(np.int16)
    return pcm16.tobytes(), sr

def stt_vosk_from_wav_bytes(wav_bytes: bytes) -> str:
    model = load_vosk_model()
    pcm_bytes, sr = wav_bytes_to_pcm16k_mono(wav_bytes)

    rec = KaldiRecognizer(model, sr)
    rec.SetWords(False)

    chunk_size = 4000
    for i in range(0, len(pcm_bytes), chunk_size):
        rec.AcceptWaveform(pcm_bytes[i:i + chunk_size])

    result = json.loads(rec.FinalResult())
    return (result.get("text") or "").strip()

# --------------------
# Session state init
# --------------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": PERSONA}]
if "chat" not in st.session_state:
    st.session_state.chat = []
if "last_audio" not in st.session_state:
    st.session_state.last_audio = None
if "status" not in st.session_state:
    st.session_state.status = ""

# --------------------
# UI
# --------------------
st.title("ğŸ€„ ä¸­æ–‡å£è¯­ç»ƒä¹ ï½œç§äººä¸­æ–‡å¯¼å¸ˆ")
st.caption("å¯ä»¥æ‰“å­—ï¼Œä¹Ÿå¯ä»¥ğŸ™ï¸è¯´ä¸­æ–‡ã€‚æˆ‘ä¼šå¸®ä½ æŠŠè¡¨è¾¾æ”¹å¾—æ›´è‡ªç„¶ã€æ›´åƒæ¯è¯­è€…ã€‚")

with st.sidebar:
    st.subheader("å­¦ä¹ å·¥å…·")

    if st.button("ğŸ§¹ é‡æ–°å¼€å§‹å¯¹è¯", use_container_width=True):
        st.session_state.messages = [{"role": "system", "content": PERSONA}]
        st.session_state.chat = []
        st.session_state.last_audio = None
        st.session_state.status = ""
        st.rerun()

    if st.button("ğŸ”Š è¯•å¬è€å¸ˆå‘éŸ³", use_container_width=True):
        try:
            audio = speak_edge_tts_bytes("ä½ å¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä¸­æ–‡ç»ƒä¹ äº†ã€‚ä½ å¯ä»¥éšä¾¿è¯´ä¸€å¥è¯ã€‚")
            st.session_state.last_audio = audio
            st.success("è¯­éŸ³å·²ç”Ÿæˆï¼ˆå¦‚æœæ²¡è‡ªåŠ¨æ’­æ”¾ï¼Œç‚¹ä¸€ä¸‹æ’­æ”¾é”®ï¼‰")
            st.audio(audio, format="audio/mpeg", autoplay=True)
        except Exception as e:
            st.error(f"TTS Error: {e}")

    with st.expander("æŠ€æœ¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰"):
        st.write("OpenRouter key loaded:", bool(OPENROUTER_API_KEY))
        st.write("Model:", OPENROUTER_MODEL)
        st.write("Edge voice:", EDGE_VOICE)
        st.write("Vosk path:", VOSK_MODEL_PATH)
        st.write("Vosk exists:", os.path.isdir(VOSK_MODEL_PATH))
        if st.button("Test OpenRouter"):
            code, body = openrouter_ping()
            st.write("Status:", code)
            st.code(body)

# Chat history
for m in st.session_state.chat:
    with st.chat_message("user" if m["role"] == "user" else "assistant"):
        st.markdown(m["content"])

# Status + audio
if st.session_state.status:
    st.info(st.session_state.status)

if st.session_state.last_audio:
    st.audio(st.session_state.last_audio, format="audio/mpeg", autoplay=True)

# --------------------
# Voice input (Press to speak) -> STT -> chat
# --------------------
st.markdown("### ğŸ™ï¸ å£è¯­ç»ƒä¹ ï¼ˆæŒ‰ä¸‹å½•éŸ³ï¼Œè¯´å®Œåœæ­¢ï¼‰")

mic = mic_recorder(
    start_prompt="ğŸ™ï¸ å¼€å§‹å½•éŸ³",
    stop_prompt="â¹ï¸ åœæ­¢",
    just_once=True,
    use_container_width=True,
    format="wav",  # critical for iOS/Safari + soundfile
)

def handle_user_message(text: str):
    st.session_state.chat.append({"role": "user", "content": text})
    st.session_state.status = "è€å¸ˆåœ¨æ€è€ƒâ€¦"
    st.session_state.last_audio = None

    try:
        urls = extract_urls(text)

        with st.spinner("è€å¸ˆåœ¨æ•´ç†ä½ çš„è¡¨è¾¾â€¦"):
            if urls:
                st.session_state.status = "æ­£åœ¨è¯»å–é“¾æ¥å†…å®¹â€¦"
                content = fetch_and_extract(urls[0])
                prompt = f"""
è¯·ä½œä¸ºä¸€åä¸­æ–‡è€å¸ˆæ¥å›å¤ï¼Œå¹¶ç»“åˆæˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯èƒŒæ™¯ã€‚

ç”¨æˆ·è¿™æ¬¡çš„è¯ï¼š
{text}

å¦‚æœç”¨æˆ·æ˜¯åœ¨é—®é“¾æ¥å†…å®¹ï¼Œè¯·å…ˆç”¨ä¸­æ–‡ç®€è¦æ€»ç»“é“¾æ¥é‡ç‚¹ï¼Œå†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¹¶æŒ‰ä»¥ä¸‹æ•™å­¦æ–¹å¼è¾“å‡ºï¼š
1) æ›´è‡ªç„¶çš„è¯´æ³•ï¼ˆå¦‚éœ€è¦ï¼‰
2) ç®€çŸ­è§£é‡Šï¼ˆç”¨ç®€å•ä¸­æ–‡ï¼‰
3) 1-2 ä¸ªå¯æ›¿æ¢è¡¨è¾¾ï¼ˆå¯é€‰ï¼‰
å›ç­”é€‚åˆæœ—è¯»ï¼Œåˆ«å¤ªé•¿ã€‚

ã€ç½‘é¡µæ­£æ–‡å¼€å§‹ã€‘
{content}
ã€ç½‘é¡µæ­£æ–‡ç»“æŸã€‘
"""
                reply = ask_openrouter(prompt)
            else:
                prompt = f"""
è¯·ä½œä¸ºä¸€åä¸­æ–‡è€å¸ˆæ¥å›å¤ã€‚

ç”¨æˆ·åŸè¯ï¼š
{text}

è¦æ±‚ï¼š
1) å¦‚æœè¡¨è¾¾ä¸è‡ªç„¶ï¼Œå…ˆç»™æ›´è‡ªç„¶çš„è¯´æ³•
2) ç”¨ç®€å•ä¸­æ–‡è§£é‡ŠåŸå› 
3) å¦‚æœè¡¨è¾¾å·²ç»å¾ˆå¥½ï¼Œè¯·æ˜ç¡®è¡¨æ‰¬
4) ç»™ 1-2 ä¸ªå¯æ›¿æ¢è¡¨è¾¾ï¼ˆå¯é€‰ï¼‰
5) å›ç­”é€‚åˆæœ—è¯»ï¼Œä¸è¦å¤ªé•¿
"""
                reply = ask_openrouter(prompt)

        st.session_state.chat.append({"role": "assistant", "content": reply})

        st.session_state.status = "æ­£åœ¨ç”Ÿæˆè¯­éŸ³â€¦"
        SAFE_TTS_CHARS = 800
        tts_text = clean_for_tts(reply[:SAFE_TTS_CHARS])
        audio = speak_edge_tts_bytes(tts_text)
        st.session_state.last_audio = audio
        st.session_state.status = ""

        st.audio(audio, format="audio/mpeg", autoplay=True)

    except Exception as e:
        st.session_state.status = ""
        st.error(f"Error: {e}")

# If mic recorded something, transcribe and send to chat
if mic and mic.get("bytes"):
    st.session_state.status = "æ­£åœ¨è¯†åˆ«è¯­éŸ³â€¦"
    try:
        spoken_text = stt_vosk_from_wav_bytes(mic["bytes"])
        st.session_state.status = ""
        if spoken_text:
            st.info(f"ğŸ—£ï¸ ä½ è¯´ï¼š{spoken_text}")
            handle_user_message(spoken_text)
            st.rerun()
        else:
            st.warning("æˆ‘æ²¡å¬æ¸…æ¥šï¼Œå†è¯•ä¸€æ¬¡ï¼Ÿ")
    except Exception as e:
        st.session_state.status = ""
        st.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼š{e}")

# --------------------
# Text input (still supported)
# --------------------
user_text = st.chat_input("è¯•ç€è¯´ä¸€å¥ä¸­æ–‡ï¼Œæˆ–ç›´æ¥æŒ‰ğŸ™ï¸è¯´è¯â€¦")
if user_text:
    handle_user_message(user_text)
    st.rerun()

# First greeting
if len(st.session_state.chat) == 0:
    st.session_state.chat.append(
        {
            "role": "assistant",
            "content": (
                "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„ä¸­æ–‡è€å¸ˆã€‚\n\n"
                "ä½ å¯ä»¥æ‰“å­—èŠå¤©ï¼Œä¹Ÿå¯ä»¥æŒ‰ğŸ™ï¸è¯´ä¸­æ–‡ã€‚\n"
                "æˆ‘ä¼šå¸®ä½ æŠŠè¡¨è¾¾æ”¹å¾—æ›´è‡ªç„¶ï¼Œå¹¶ç®€å•è§£é‡ŠåŸå› ã€‚\n\n"
                "æˆ‘ä»¬å…ˆçƒ­èº«ä¸€å¥ï¼š\n"
                "ğŸ‘‰ã€Œæˆ‘ä»Šå¤©æœ‰ç‚¹å¿™ï¼Œä½†æ˜¯å¿ƒæƒ…ä¸é”™ã€‚ã€\n"
                "ä½ ä¹Ÿå¯ä»¥ç”¨è‡ªå·±çš„è¯è¯´ä¸€å¥ã€‚"
            ),
        }
    )
    st.rerun()
