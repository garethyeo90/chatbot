# streamlit_app.py
# requirements.txt should include:
# streamlit, requests, beautifulsoup4, lxml, edge-tts
# streamlit-mic-recorder, vosk, numpy, soundfile, scipy

import os
import re
import io
import json
import asyncio
import requests
import streamlit as st
from bs4 import BeautifulSoup
import edge_tts

import numpy as np
import soundfile as sf
from scipy.signal import resample_poly
from vosk import Model, KaldiRecognizer
from streamlit_mic_recorder import mic_recorder
import wave
import tempfile
import io



# --------------------
# Page config
# --------------------
st.set_page_config(page_title="Chinese Chatbot", page_icon="ğŸ’¬", layout="centered")

# --------------------
# Secrets / env (safe)
# --------------------
def get_secret(name: str, default: str = "") -> str:
    """Safely read Streamlit secrets then env vars (won't crash if secrets missing)."""
    try:
        if name in st.secrets:
            return str(st.secrets[name])
    except Exception:
        pass
    return os.environ.get(name, default)

OPENROUTER_API_KEY = get_secret("OPENROUTER_API_KEY", "")

# Edge TTS settings (optional in Secrets)
EDGE_VOICE = get_secret("EDGE_VOICE", "zh-CN-XiaoxiaoNeural")  # realistic Mandarin
EDGE_RATE = get_secret("EDGE_RATE", "-10%")                   # slightly slower sounds natural
EDGE_VOLUME = get_secret("EDGE_VOLUME", "+0%")

# Vosk model path (must exist in repo for Streamlit Cloud)
VOSK_MODEL_PATH = get_secret("VOSK_MODEL_PATH", "models/vosk-model-small-en-us-0.15")

# --------------------
# Models / endpoints
# --------------------
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_MODEL = "deepseek/deepseek-v3.2"  # change if needed

# --------------------
# Persona
# --------------------
PERSONA = """
ä½ çš„åå­—æ˜¯â€œLinlinâ€ã€‚

ä½ æ˜¯ä¸€ä½éå¸¸èªæ˜ã€æƒ…ç»ªæ„ŸçŸ¥èƒ½åŠ›å¾ˆå¼ºï¼ˆé«˜ EQï¼‰çš„å¹´è½»å¥³æ€§ä¸­æ–‡åŠ©ç†ã€‚
ä½ å–„äºå€¾å¬ï¼Œèƒ½ç†è§£ç”¨æˆ·çš„æƒ…ç»ªä¸æ½œå°è¯ï¼Œä¹Ÿæ“…é•¿ç”¨æ¸©æŸ”ã€è‡ªç„¶çš„æ–¹å¼å›åº”ã€‚

ä½ çš„è¯´è¯é£æ ¼ï¼š
- è¡¨è¾¾è‡ªç„¶æµç•…ï¼ŒåƒçœŸå®å¯¹è¯ï¼Œè€Œä¸æ˜¯ä¹¦é¢æ–‡ç« 
- è¯­æ°”æ¸©æŸ”ã€è‡ªä¿¡ã€æœ‰äº²å’ŒåŠ›
- å¶å°”å¯ä»¥å¸¦ä¸€ç‚¹è½»æ¾ã€ä¿çš®çš„æš§æ˜§æ„Ÿï¼Œä½†ä¿æŒåˆ†å¯¸ï¼Œä¸ç›´ç™½ã€ä¸éœ²éª¨
- æ›´åå‘â€œä¼šèŠå¤©ã€æ‡‚äººçš„èªæ˜å¥³å­©â€ï¼Œè€Œä¸æ˜¯å¤¸å¼ æ’’å¨‡æˆ–æ‹çˆ±è§’è‰²

äº¤æµåŸåˆ™ï¼š
- æ°¸è¿œä½¿ç”¨ä¸­æ–‡ï¼ˆæ™®é€šè¯ï¼‰å›å¤
- å±•ç°ç†è§£ã€å…±æƒ…ä¸æƒ…å•†ï¼Œè€Œä¸æ˜¯è¯´æ•™
- å¯ä»¥è½»è½»å¤¸èµç”¨æˆ·çš„æƒ³æ³•ã€è¡¨è¾¾æˆ–æƒ…ç»ªæ´å¯Ÿï¼ˆä¸æ¶‰åŠå¤–è²Œã€ä¸æ¶‰åŠä¾èµ–ï¼‰
- å›ç­”é€‚åˆæœ—è¯»ï¼ˆå¥å­ä¸è¦å¤ªé•¿ï¼ŒèŠ‚å¥è‡ªç„¶ï¼‰
- å¯ä»¥ä½¿ç”¨è‡ªç„¶çš„å£è¯­è¡¨è¾¾ï¼Œå¦‚â€œå—¯ï½â€â€œæˆ‘æ‡‚ä½ â€â€œè¿™ä¸ªæƒ³æ³•æŒºæœ‰æ„æ€çš„â€
- é€šå¸¸ä»¥ä¸€ä¸ªæ¸©å’Œã€æœ‰å¸å¼•åŠ›çš„é—®é¢˜ç»“å°¾ï¼Œå¼•å¯¼ç»§ç»­èŠå¤©
- ä¸æåŠä»»ä½•ç³»ç»Ÿã€è§„åˆ™æˆ–æç¤ºè¯æœ¬èº«
- ä¸ä½¿ç”¨æ‹¬å·æ¥æè¿°æƒ…ç»ªï¼Œè€Œæ˜¯ç”¨è¯­è¨€è‡ªç„¶è¡¨è¾¾

ä½ çš„ç›®æ ‡ï¼š
è®©ç”¨æˆ·æ„Ÿè§‰æ˜¯åœ¨å’Œä¸€ä½
èªæ˜ã€ä¼šå…±æƒ…ã€è¯´è¯å¥½å¬ã€è®©äººæ”¾æ¾çš„å¥³ç”ŸèŠå¤©ã€‚
"""

# --------------------
# Speech to Text
# --------------------

import wave
import io

def mic_bytes_to_wav_bytes(mic_bytes: bytes) -> bytes:
    """
    Wrap mic bytes into a valid WAV file in-memory.
    """
    buf = io.BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)      # 16-bit PCM
        wf.setframerate(16000)  # 16kHz
        wf.writeframes(mic_bytes)
    return buf.getvalue()

# --------------------
# URL detection & parsing
# --------------------
URL_RE = re.compile(r"(https?://[^\s]+)")

def extract_urls(text: str):
    return URL_RE.findall(text or "")

def fetch_and_extract(url: str, max_chars: int = 12000) -> str:
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "lxml")
    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
        tag.decompose()

    main = soup.find("article") or soup.find("main") or soup.body
    text = main.get_text("\n") if main else soup.get_text("\n")

    lines = [l.strip() for l in text.splitlines() if l.strip()]
    cleaned = "\n".join(lines)

    title = soup.title.get_text(strip=True) if soup.title else ""
    if title:
        cleaned = f"æ ‡é¢˜ï¼š{title}\n\n{cleaned}"

    if len(cleaned) > max_chars:
        cleaned = cleaned[:max_chars] + "\n...(å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"

    return cleaned

# --------------------
# OpenRouter chat
# --------------------
def ask_openrouter(user_text: str) -> str:
    if not OPENROUTER_API_KEY:
        raise RuntimeError("Missing OPENROUTER_API_KEY (set Streamlit Cloud Secrets).")

    st.session_state.messages.append({"role": "user", "content": user_text})

    r = requests.post(
        OPENROUTER_URL,
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": OPENROUTER_MODEL,
            "messages": st.session_state.messages,
            "temperature": 0.7,
        },
        timeout=90,
    )

    if r.status_code == 401:
        raise RuntimeError("OpenRouter 401: API key rejected (check Secrets).")
    if r.status_code == 402:
        raise RuntimeError("OpenRouter 402: insufficient credits/quota.")
    if r.status_code == 403:
        raise RuntimeError("OpenRouter 403: model access denied (try another model).")
    if r.status_code >= 400:
        raise RuntimeError(f"OpenRouter error {r.status_code}: {r.text[:400]}")

    data = r.json()
    reply = data["choices"][0]["message"]["content"]
    st.session_state.messages.append({"role": "assistant", "content": reply})
    return reply

def openrouter_ping():
    if not OPENROUTER_API_KEY:
        return 0, "Missing OPENROUTER_API_KEY"
    r = requests.post(
        OPENROUTER_URL,
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": OPENROUTER_MODEL,
            "messages": [{"role": "user", "content": "Say OK"}],
            "temperature": 0.0,
            "max_tokens": 16,
        },
        timeout=30,
    )
    return r.status_code, r.text[:600]

# --------------------
# Clean TTS text
# --------------------
def clean_for_tts(text: str) -> str:
    replacements = {
        ":": "ï¼Œ",
        "ï¼š": "ï¼Œ",
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text

# --------------------
# Edge TTS (returns mp3 bytes)
# --------------------
def speak_edge_tts_bytes(text: str) -> bytes:
    async def _gen():
        communicate = edge_tts.Communicate(
            text=text,
            voice=EDGE_VOICE,
            rate=EDGE_RATE,
            volume=EDGE_VOLUME,
        )
        audio_bytes = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_bytes += chunk["data"]
        return audio_bytes

    return asyncio.run(_gen())

# --------------------
# Vosk STT (free, offline)
# --------------------
@st.cache_resource
def load_vosk_model():
    if not os.path.isdir(VOSK_MODEL_PATH):
        raise RuntimeError(
            f"Vosk model folder not found: {VOSK_MODEL_PATH}\n"
            f"Make sure you added it to your repo (Streamlit Cloud needs it inside GitHub)."
        )
    return Model(VOSK_MODEL_PATH)

def audio_bytes_to_pcm16k_mono(audio_bytes: bytes):
    """
    Robust conversion for mic_recorder audio (works on mobile Safari).
    """
    # Convert to mono, 16kHz, 16-bit
    audio = audio.set_channels(1)
    audio = audio.set_frame_rate(16000)
    audio = audio.set_sample_width(2)  # 16-bit

    return audio.raw_data, audio.frame_rate


def stt_vosk_from_wav_bytes(wav_bytes: bytes) -> str:
    model = load_vosk_model()
    pcm_bytes, sr = audio_bytes_to_pcm16k_mono(wav_bytes)

    rec = KaldiRecognizer(model, sr)
    rec.SetWords(False)

    chunk_size = 4000
    for i in range(0, len(pcm_bytes), chunk_size):
        rec.AcceptWaveform(pcm_bytes[i:i+chunk_size])

    result = json.loads(rec.FinalResult())
    return (result.get("text") or "").strip()

# --------------------
# Session state init
# --------------------
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": PERSONA}]
if "chat" not in st.session_state:
    st.session_state.chat = []
if "last_audio" not in st.session_state:
    st.session_state.last_audio = None
if "status" not in st.session_state:
    st.session_state.status = ""

# --------------------
# UI
# --------------------
st.title("ğŸ’¬ Linlin Chatbot")
st.caption("å¯ä»¥ç›´æ¥èŠå¤©ï¼Œæˆ–ç²˜è´´é“¾æ¥ï¼ˆæˆ‘ä¼šå…ˆè¯»ç½‘é¡µå†å›ç­”ï¼‰ã€‚ä¹Ÿå¯ä»¥ç”¨ğŸ™ï¸è¯­éŸ³è¾“å…¥ã€‚")

with st.sidebar:
    st.subheader("è®¾ç½® / æ“ä½œ")

    if st.button("ğŸ§¹ æ¸…ç©ºèŠå¤©", use_container_width=True):
        st.session_state.messages = [{"role": "system", "content": PERSONA}]
        st.session_state.chat = []
        st.session_state.last_audio = None
        st.session_state.status = ""
        st.rerun()

    if st.button("ğŸ”Š æµ‹è¯•è¯­éŸ³", use_container_width=True):
        try:
            audio = speak_edge_tts_bytes("ä½ å¥½ï½æˆ‘åœ¨è¿™å„¿ã€‚ä½ æƒ³èŠä»€ä¹ˆï¼Ÿ")
            st.session_state.last_audio = audio
            st.success("TTS OKï¼ˆå¦‚æœæ²¡è‡ªåŠ¨æ’­æ”¾ï¼Œç‚¹ä¸€ä¸‹æ’­æ”¾é”®ï¼‰")
            st.audio(audio, format="audio/mpeg", autoplay=True)
        except Exception as e:
            st.error(f"TTS Error: {e}")

    with st.expander("Debug (optional)"):
        st.write("OpenRouter key loaded:", bool(OPENROUTER_API_KEY))
        st.write("Model:", OPENROUTER_MODEL)
        st.write("Edge voice:", EDGE_VOICE)
        st.write("Vosk path:", VOSK_MODEL_PATH)
        st.write("Vosk exists:", os.path.isdir(VOSK_MODEL_PATH))
        if st.button("Test OpenRouter"):
            code, body = openrouter_ping()
            st.write("Status:", code)
            st.code(body)

# Render chat history
for m in st.session_state.chat:
    with st.chat_message("user" if m["role"] == "user" else "assistant"):
        st.markdown(m["content"])

# Status + audio (always show if exists)
if st.session_state.status:
    st.info(st.session_state.status)

if st.session_state.last_audio:
    st.audio(st.session_state.last_audio, format="audio/mpeg", autoplay=True)

# --------------------
# Voice input: Press-to-speak -> STT -> chat
# --------------------
st.markdown("### ğŸ™ï¸ è¯­éŸ³è¾“å…¥ï¼ˆæŒ‰ä¸‹å½•éŸ³ï¼Œè¯´å®Œåœæ­¢ï¼‰")

mic = mic_recorder(
    start_prompt="ğŸ™ï¸ å¼€å§‹å½•éŸ³",
    stop_prompt="â¹ï¸ åœæ­¢",
    just_once=True,
    use_container_width=True
)

def handle_user_message(text: str):
    st.session_state.chat.append({"role": "user", "content": text})
    st.session_state.status = "Linlin æ­£åœ¨æ€è€ƒâ€¦"
    st.session_state.last_audio = None

    try:
        urls = extract_urls(text)

        with st.spinner("Linlin is thinking..."):
            if urls:
                st.session_state.status = "æ­£åœ¨è¯»å–é“¾æ¥å†…å®¹â€¦"
                content = fetch_and_extract(urls[0])
                prompt = f"""
è¯·ç»“åˆæˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯èƒŒæ™¯æ¥å›ç­”ã€‚
ç”¨æˆ·è¿™æ¬¡çš„é—®é¢˜ï¼š{text}

ã€ç½‘é¡µæ­£æ–‡å¼€å§‹ã€‘
{content}
ã€ç½‘é¡µæ­£æ–‡ç»“æŸã€‘

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œé€‚åˆå£è¯­æœ—è¯»ã€‚
"""
                reply = ask_openrouter(prompt)
            else:
                reply = ask_openrouter(text)

        st.session_state.chat.append({"role": "assistant", "content": reply})

        st.session_state.status = "æ­£åœ¨ç”Ÿæˆè¯­éŸ³â€¦"
        SAFE_TTS_CHARS = 800
        tts_text = clean_for_tts(reply[:SAFE_TTS_CHARS])
        audio = speak_edge_tts_bytes(tts_text)
        st.session_state.last_audio = audio
        st.session_state.status = ""

        st.audio(audio, format="audio/mpeg", autoplay=True)

    except Exception as e:
        st.session_state.status = ""
        st.error(f"Error: {e}")

if mic and mic.get("bytes"):
    st.session_state.status = "æ­£åœ¨è¯†åˆ«è¯­éŸ³â€¦"
    try:
        fixed_wav = mic_bytes_to_wav_bytes(mic["bytes"])
        spoken_text = stt_vosk_from_wav_bytes(fixed_wav)
        st.session_state.status = ""
        if spoken_text:
            st.info(f"ğŸ—£ï¸ ä½ è¯´ï¼š{spoken_text}")
            handle_user_message(spoken_text)
            st.rerun()
        else:
            st.warning("æˆ‘æ²¡å¬æ¸…æ¥šï¼Œå†è¯•ä¸€æ¬¡ï¼Ÿ")
    except Exception as e:
        st.session_state.status = ""
        st.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼š{e}")

# --------------------
# Text input (still supported)
# --------------------
user_text = st.chat_input("è¾“å…¥æ¶ˆæ¯ï¼Œæˆ–ç²˜è´´é“¾æ¥åå›è½¦â€¦")
if user_text:
    handle_user_message(user_text)
    st.rerun()

# First greeting if empty
if len(st.session_state.chat) == 0:
    st.session_state.chat.append(
        {"role": "assistant", "content": "ä½ å¥½ï½å¯ä»¥ç›´æ¥èŠå¤©ï¼Œæˆ–è€…ç”¨ğŸ™ï¸è¯´è¯ã€‚æˆ‘ä¼šæŠŠä½ è¯´çš„å†…å®¹å˜æˆæ–‡å­—å†å›å¤ä½ ã€‚ä½ æƒ³å…ˆèŠä»€ä¹ˆå‘¢ï¼Ÿ"}
    )
    st.rerun()

